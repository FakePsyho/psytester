[general]
# version, don't edit this unless you're trying to manually update the config file to a newer version of mmtester
version = 0.3.0

# dir where result files are saved
results_dir = .

# dir where outputs are saved
tests_dir = tests

# if set to False all outputs are saved to separate folders (tests_dir/run_name/), if set to True, they are saved to the same folder (tests_dir/)
merge_output_dirs = False

# extension for result files
results_ext = .res

# command in order to run a single test
run_cmd = java -jar tester.jar 

# novis parameter
run_no_vis = -novis

# whether keep the outputs
keep_output_files = True

# number of precision digits used in scoring
precision = 4

# additional column for showing number of best scores achieved
show_bests = True

# additional column for showing number of best uniques scores achieved
show_uniques = True

# additional column for showing gain (how much score the solution contributes towards maximums)
show_gain = True

# automatically hides column for number of fails if there are no failed test cases
autohide_fails = False


[default]
# type of scoring: raw (sum of absolute scores), min (relative, lower = better), max (relative, higher = better)
scoring = max

# number of concurrent runs 
threads_no = 3

# whether to show current progress
progress = True

# either number of tests (seeds 1-N), range of seeds (A-B) or the name of the file that stores seeds in plain text (1 seed per line) or JSON format used in results files
tests = 

# path to executable for the solution
exec = a.exe

# type of topcoder tester: old (pre 2021 contests), new (new testers that support multithreading etc.), newrt (same as new but also extracts run time); set it to new/newrt if possible, since it solves the problem of potentially mangling together solution's stderr and tester's stdout
tc_tester = newrt

# no idea why it's not the part of run_cmd ;)
tester_arguments = 

# path to file with metadata (including extension); set it to LATEST if you always want to use latest res file
data = LATEST

# scales scores so that max is equal to scale no matter the number of tests, leave it empty to just show the sum
scale = 100.0

# how to sort results files: name (alphabetically), date (oldest to newest)
sorting = name

# path to benchmark, allows for quick comparison of results while the test is running; progress has to be set to true
benchmark =

[scripts]
# used when running --generate-scripts, each line represent a single script file to generate
# "\n" will be automatically replaced with a newline
# some variable that you can use:
# %RUN_CMD% -> general/run_cmd
# %EXEC% -> EXEC (--exec EXEC) or default/exec 
# %SOURCE% -> SOURCE (--source SOURCE)
# %IP% -> IP (--ip IP)
# if a script has an undefined variable, it won't be generated (it will issue a warning instead)

c.bat = g++ -O3 -std=gnu++11 %SOURCE% -o a.exe
v.bat = %RUN_CMD% -exec %EXEC% -seed %*
n.bat = %RUN_CMD% -exec %EXEC% -novis -seed %*
s.bat = mmtester --show %*
r.bat = cp %SOURCE% backup/%1.cpp\nmmtester -m 2 -t 100 %1
rr.bat = cp %SOURCE% backup/%1.cpp\nscp %SOURCE% ubuntu@%IP%:~/ && ssh ubuntu@%IP% g++ -O3 %SOURCE% -o ./a.out && ssh ubuntu@%IP% python3 /home/ubuntu/.local/lib/python3.8/site-packages/mmtester/tester.py -m 40 -t 1-2000 -e ./a.out %1 && scp ubuntu@%IP%:~/%1.res . && s


